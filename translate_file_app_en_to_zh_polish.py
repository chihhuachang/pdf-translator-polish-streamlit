import streamlit as st
import google.generativeai as genai
import os
import sys
import io
import time

# Import libraries for file reading AND writing docx
import docx
from docx import Document
import PyPDF2
from PyPDF2.errors import PdfReadError

# --- å¸¸æ•¸å®šç¾© (ä¿æŒä¸è®Š) ---
MAX_CHARS_PER_CHUNK = 2500
API_CALL_DELAY = 2 # ç§’

# --- èªªæ˜æ–‡ä»¶ ---
# åŠŸèƒ½ï¼šä¸Šå‚³è‹±æ–‡æ–‡ä»¶ï¼Œåˆ†å¡Šç¿»è­¯æˆç¹é«”ä¸­æ–‡ï¼Œåˆä½µå¾Œï¼Œ
#       å†å°åˆä½µçµæœé€²è¡Œ **æ½¤é£¾èˆ‡æ ¡å°**ï¼Œä¸¦åˆ†åˆ¥æä¾›åˆæ­¥ç¿»è­¯å’Œæ½¤é£¾å¾Œç¿»è­¯çš„ Docx ä¸‹è¼‰ã€‚
# (å…¶é¤˜èªªæ˜èˆ‡ä¹‹å‰ç‰ˆæœ¬ç›¸åŒ)
#
# å¦‚ä½•åŸ·è¡Œï¼š
# 1. å®‰è£å‡½å¼åº«: pip install streamlit google-generativeai python-docx PyPDF2
# 2. å°‡æ­¤ç¨‹å¼ç¢¼å„²å­˜ç‚º `translate_file_app_en_to_zh_polish.py`ã€‚
# 3. åœ¨çµ‚ç«¯æ©Ÿä¸­åŸ·è¡Œï¼š streamlit run translate_file_app_en_to_zh_polish.py
# ---

# --- API é‡‘é‘°è¨­å®š (ä¿æŒä¸è®Š) ---
api_key = os.getenv("GOOGLE_API_KEY")
# ...(çœç•¥ API Key æª¢æŸ¥èˆ‡è¨­å®šç¨‹å¼ç¢¼)...
if not api_key: st.error("..."); st.stop()
try: genai.configure(api_key=api_key)
except Exception as e: st.error(f"...: {e}"); st.stop()

# --- å¾æª”æ¡ˆæå–æ–‡å­—çš„å‡½å¼ (ä¿æŒä¸è®Š) ---
def extract_text_from_file(uploaded_file):
    """ (æ­¤å‡½å¼é‚è¼¯èˆ‡ä¸Šä¸€ç‰ˆæœ¬å®Œå…¨ç›¸åŒ) """
    extracted_text = ""
    # ...(çœç•¥æª”æ¡ˆè®€å–å’ŒéŒ¯èª¤è™•ç†é‚è¼¯)...
    try:
        file_extension = os.path.splitext(uploaded_file.name)[1].lower()
        if file_extension == ".txt":
            try: extracted_text = uploaded_file.getvalue().decode("utf-8")
            except UnicodeDecodeError:
                st.warning("å˜—è©¦ UTF-8 è§£ç¢¼å¤±æ•—ï¼Œå˜—è©¦ Big5...")
                try: extracted_text = uploaded_file.getvalue().decode("big5", errors='ignore')
                except Exception as e_enc:
                     st.error(f"å˜—è©¦ Big5 è§£ç¢¼ä¹Ÿå¤±æ•—: {e_enc}ã€‚ä½¿ç”¨å¿½ç•¥éŒ¯èª¤çš„ UTF-8ã€‚")
                     extracted_text = uploaded_file.getvalue().decode("utf-8", errors='ignore')
            st.info(f"æˆåŠŸè®€å– .txt æª”æ¡ˆ: {uploaded_file.name}")
        elif file_extension == ".docx":
            document = docx.Document(uploaded_file)
            extracted_text = '\n'.join([para.text for para in document.paragraphs])
            st.info(f"æˆåŠŸè®€å– .docx æª”æ¡ˆ: {uploaded_file.name}")
        elif file_extension == ".pdf":
            try:
                pdf_reader = PyPDF2.PdfReader(uploaded_file)
                if pdf_reader.is_encrypted: st.error("éŒ¯èª¤ï¼šPDF æ–‡ä»¶å·²åŠ å¯†ã€‚"); return None
                full_text = [page.extract_text() for i, page in enumerate(pdf_reader.pages) if page.extract_text() or st.warning(f"è®€å– PDF ç¬¬ {i+1} é æ™‚æœªæå–åˆ°æ–‡å­—æˆ–ç™¼ç”ŸéŒ¯èª¤ã€‚", icon="âš ï¸")]
                extracted_text = '\n'.join(filter(None, full_text))
                if not extracted_text.strip(): st.warning(f"ç„¡æ³•å¾ PDF '{uploaded_file.name}' æå–ä»»ä½•æ–‡å­—ã€‚")
                else: st.info(f"æˆåŠŸè®€å– .pdf æª”æ¡ˆ: {uploaded_file.name}")
            except PdfReadError as pdf_err: st.error(f"PyPDF2 è®€å–éŒ¯èª¤: {pdf_err}"); return None
        else: st.error(f"éŒ¯èª¤ï¼šä¸æ”¯æ´çš„æª”æ¡ˆé¡å‹ '{file_extension}'ã€‚"); return None
        return extracted_text.strip()
    except Exception as e: st.error(f"è®€å–æˆ–è§£ææª”æ¡ˆ '{uploaded_file.name}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"); return None


# --- æ–‡æœ¬åˆ†å¡Šå‡½å¼ (ä¿æŒä¸è®Š) ---
def split_text_into_chunks(text, max_chars=MAX_CHARS_PER_CHUNK):
    """ (æ­¤å‡½å¼é‚è¼¯èˆ‡ä¸Šä¸€ç‰ˆæœ¬å®Œå…¨ç›¸åŒ) """
    chunks = []
    # ...(çœç•¥åˆ†å¡Šé‚è¼¯)...
    paragraphs = text.split('\n\n')
    current_chunk = ""
    for paragraph in paragraphs:
        if len(paragraph) > max_chars:
            start = 0
            while start < len(paragraph):
                end = min(start + max_chars, len(paragraph))
                long_para_chunk = paragraph[start:end]
                if len(current_chunk) + len(long_para_chunk) + 2 <= max_chars:
                     if current_chunk: current_chunk += "\n\n" + long_para_chunk
                     else: current_chunk = long_para_chunk
                else:
                    if current_chunk: chunks.append(current_chunk)
                    current_chunk = long_para_chunk
                    if len(current_chunk) >= max_chars: chunks.append(current_chunk); current_chunk = ""
                start = end
        else:
            if len(current_chunk) + len(paragraph) + 2 <= max_chars:
                if current_chunk: current_chunk += "\n\n" + paragraph
                else: current_chunk = paragraph
            else:
                chunks.append(current_chunk)
                current_chunk = paragraph
    if current_chunk: chunks.append(current_chunk)
    return chunks

# --- ç¿»è­¯å‡½å¼ (ä¿æŒä½¿ç”¨å›ºå®šçš„è©³ç´°æç¤ºè©å’Œæµå¼å‚³è¼¸) ---
def translate_text(text_to_translate, target_language="ç¹é«”ä¸­æ–‡"):
    """ (æ­¤å‡½å¼é‚è¼¯èˆ‡ä¸Šä¸€ç‰ˆæœ¬å®Œå…¨ç›¸åŒï¼Œä½¿ç”¨ä½ æä¾›çš„å›ºå®šæç¤ºè©) """
    if not text_to_translate: return None
    fixed_instruction_prompt = """
Please act as a professional Chinese translator and you will actually follow the steps below to produce a natural and professional Chinese translation.
1. Carefully read and fully understand the original text, ensuring thorough comprehension without haste.
2. Carefully think and consider how you would share the content you just read with your imagined audience in Chinese.
3. Start to translate the text by writting down the proposed sharing content you just had with your imagined audience using traditional Chinese characters. Avoid translating word-for-word; aim for a comfortable, natural, and smooth manner of expression.
""".strip()
    full_prompt = f"{fixed_instruction_prompt}\n\n{text_to_translate}"
    model = genai.GenerativeModel('gemini-1.5-flash-latest')
    try:
        response = model.generate_content(full_prompt, stream=True)
        full_translated_text = "".join(chunk.text for chunk in response if hasattr(chunk, 'text') and chunk.text)
        return full_translated_text.strip()
    except Exception as e:
        st.error(f"ç¿»è­¯å¡Š '{text_to_translate[:30]}...' æ™‚ API å‘¼å«æˆ–æµå¼è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        return f"[[ç¿»è­¯éŒ¯èª¤æ–¼å¡Š: {text_to_translate[:30]}... - {e}]]"


# --- æ–°å¢ï¼šæ½¤é£¾ç¿»è­¯æ–‡æœ¬å‡½å¼ ---
def polish_translation(raw_translated_text, target_language="ç¹é«”ä¸­æ–‡"):
    """
    ä½¿ç”¨ Gemini API å°åˆæ­¥ç¿»è­¯çš„æ–‡æœ¬é€²è¡Œåˆ†æã€æ ¡å°ã€å»é™¤è´…å­—èˆ‡èªæ³•æ½¤é£¾ã€‚
    """
    if not raw_translated_text or not raw_translated_text.strip():
        st.warning("æ²’æœ‰å¯æ½¤é£¾çš„åˆæ­¥ç¿»è­¯å…§å®¹ã€‚")
        return None

    # å°ˆç‚ºæ½¤é£¾è¨­è¨ˆçš„æç¤ºè©
    polishing_prompt_instruction = f"""
ä»¥ä¸‹æ˜¯ä¸€æ®µç”±è‹±æ–‡é€å¡Šç¿»è­¯å†åˆä½µè€Œæˆçš„{target_language}æ–‡æœ¬ã€‚
ç”±æ–¼æ˜¯åˆ†å¡Šç¿»è­¯å¾Œåˆä½µï¼Œå¯èƒ½å­˜åœ¨ä»¥ä¸‹å•é¡Œï¼š
1.  èªå¥ä¹‹é–“éŠœæ¥ä¸å¤ è‡ªç„¶æµæš¢ã€‚
2.  å¯èƒ½å‡ºç¾å› åˆ†å¡Šå°è‡´çš„èªæ„ä¸­æ–·æˆ–é‡è¤‡ã€‚
3.  å¯èƒ½åŒ…å«æ¨¡å‹å› åˆ†å¡Šä¸å®Œæ•´è€Œç”¢ç”Ÿçš„æç¤ºæ€§èªå¥ï¼ˆä¾‹å¦‚ï¼š"The text cuts off here", "ä»¥ä¸‹ç¯‡å¹…éé•·" ç­‰é¡ä¼¼è¨Šæ¯ï¼‰ã€‚
4.  æ•´é«”é¢¨æ ¼å¯èƒ½ä¸å¤ çµ±ä¸€ã€‚

è«‹ä½ æ‰®æ¼”ä¸€ä½è³‡æ·±çš„ä¸­æ–‡ç·¨è¼¯ï¼ŒåŸ·è¡Œä»¥ä¸‹ä»»å‹™ï¼š
1.  **åˆ†æä¸¦ç†è§£**æä¾›çš„æ–‡æœ¬å…§å®¹ã€‚
2.  **æ ¡å°èªæ³•éŒ¯èª¤**ï¼Œä¿®æ­£ä»»ä½•ä¸æ­£ç¢ºçš„è¡¨é”ã€‚
3.  **å»é™¤è´…å­—å’Œé‡è¤‡**ï¼Œä½¿èªè¨€æ›´ç²¾ç…‰ã€‚
4.  **æ½¤é£¾èªå¥**ï¼Œç¢ºä¿æ•´ç¯‡æ–‡æœ¬èªæ°£é€£è²«ã€è¡¨é”è‡ªç„¶ã€æµæš¢æ˜“è®€ï¼Œç¬¦åˆå°ˆæ¥­çš„{target_language}æ›¸å¯«é¢¨æ ¼ã€‚
5.  **ç§»é™¤æˆ–ä¿®æ­£**ä»»ä½•ç”±åˆ†å¡Šç¿»è­¯ç”¢ç”Ÿçš„ä¸å¿…è¦æç¤ºæ€§èªå¥æˆ–ä¸­æ–·æ¨™è¨˜ã€‚
6.  ç¢ºä¿æœ€çµ‚è¼¸å‡ºçš„æ–‡æœ¬æ„æ€å¿ æ–¼åŸæ–‡ï¼ˆé›–ç„¶ä½ çœ‹ä¸åˆ°åŸå§‹è‹±æ–‡ï¼Œä½†è¦åŸºæ–¼æä¾›çš„ä¸­æ–‡è­¯æ–‡ä½¿å…¶æ›´å®Œç¾ï¼‰ã€‚
7.  è«‹ç›´æ¥è¼¸å‡ºæ½¤é£¾å¾Œçš„å®Œæ•´{target_language}æ–‡æœ¬ï¼Œä¸è¦åŒ…å«ä»»ä½•é¡å¤–çš„è§£é‡‹æˆ–é–‹é ­èªã€‚

å¾…æ½¤é£¾çš„{target_language}æ–‡æœ¬å¦‚ä¸‹ï¼š
""".strip()

    full_polishing_prompt = f"{polishing_prompt_instruction}\n\n{raw_translated_text}"

    # ç‚ºæ½¤é£¾ä»»å‹™é¸æ“‡æ¨¡å‹ï¼ŒPro ç³»åˆ—å¯èƒ½æ›´ä½³ï¼Œä½† Flash ä¹Ÿèƒ½å˜—è©¦
    # model = genai.GenerativeModel('gemini-1.5-pro-latest')
    model = genai.GenerativeModel('gemini-1.5-flash-latest') # ä¿æŒèˆ‡ç¿»è­¯ä¸€è‡´ï¼Œæˆ–å¯å‡ç´š

    st.info("æ­£åœ¨å°‡åˆæ­¥ç¿»è­¯çµæœå‚³é€çµ¦æ¨¡å‹é€²è¡Œæ½¤é£¾èˆ‡æ ¡å°...")

    try:
        response = model.generate_content(full_polishing_prompt, stream=True) # åŒæ¨£ä½¿ç”¨æµå¼
        polished_text = "".join(chunk.text for chunk in response if hasattr(chunk, 'text') and chunk.text)
        return polished_text.strip()
    except Exception as e:
        st.error(f"æ½¤é£¾ç¿»è­¯æ™‚ API å‘¼å«æˆ–æµå¼è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        return f"[[æ½¤é£¾éŒ¯èª¤: {e}]]"


# --- å¾æ–‡å­—å»ºç«‹ Docx æª”æ¡ˆå‡½å¼ (ä¿æŒä¸è®Š) ---
def create_docx_from_text(text_content, base_filename, suffix=""):
    """ (æ­¤å‡½å¼å¾®èª¿ï¼ŒåŠ å…¥æª”åå¾Œç¶´) """
    try:
        document = Document()
        for paragraph in text_content.split('\n'): document.add_paragraph(paragraph)
        docx_buffer = io.BytesIO()
        document.save(docx_buffer)
        docx_buffer.seek(0)
        docx_filename = f"{base_filename}{suffix}.docx" # åŠ å…¥å¾Œç¶´
        return {'name': docx_filename, 'data': docx_buffer}, None
    except Exception as e:
        error_message = f"å»ºç«‹ Docx æª”æ¡ˆ '{base_filename}{suffix}.docx' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
        st.error(error_message)
        return {'name': None, 'data': None}, error_message


# --- Streamlit æ‡‰ç”¨ç¨‹å¼ä»‹é¢ (èª¿æ•´è¼¸å‡ºå€) ---
st.set_page_config(page_title="æ–‡ä»¶ç¿»è­¯+æ½¤é£¾ (è‹±->ç¹ä¸­)", layout="wide")
st.title("ğŸ“ Gemini æ–‡ä»¶ç¿»è­¯èˆ‡æ½¤é£¾ (è‹±æ–‡ â” ç¹é«”ä¸­æ–‡)")
st.caption(f"ä¸Šå‚³æ–‡ä»¶ï¼Œåˆ†å¡Šç¿»è­¯ï¼Œåˆä½µå¾Œå†é€²è¡ŒAIæ½¤é£¾ï¼Œä¸¦æä¾› Docx ä¸‹è¼‰")

# ...(çœç•¥ col1, uploaded_file, base_filename, translate_button å®šç¾©)...
col1, col2 = st.columns([2,3]) # èª¿æ•´æ¬„ä½å¯¬åº¦æ¯”ä¾‹
uploaded_file = None
base_filename = None
with col1:
    st.subheader("æ­¥é©Ÿ 1: ä¸Šå‚³è‹±æ–‡æ–‡ä»¶")
    uploaded_file = st.file_uploader(
        "é¸æ“‡è¦ç¿»è­¯çš„è‹±æ–‡æ–‡ä»¶ (.txt, .docx, .pdf)", type=['txt', 'docx', 'pdf'], key="file_uploader"
    )
    if uploaded_file is not None:
        st.markdown(f"**å·²ä¸Šå‚³æª”æ¡ˆ:** `{uploaded_file.name}` (`{uploaded_file.type}`)")
        base_filename = os.path.splitext(uploaded_file.name)[0]
    translate_button = st.button("é–‹å§‹ç¿»è­¯èˆ‡æ½¤é£¾", key="translate_btn", disabled=uploaded_file is None)

# --- ç¿»è­¯çµæœèˆ‡æ½¤é£¾çµæœè¼¸å‡ºå€ ---
with col2:
    st.subheader("åˆæ­¥ç¿»è­¯çµæœ (ç¹é«”ä¸­æ–‡)")
    raw_result_placeholder = st.empty()
    raw_result_placeholder.text_area(
        label="raw_translation_label", value="åˆæ­¥ç¿»è­¯çµæœå°‡é¡¯ç¤ºæ–¼æ­¤...", height=250,
        key="raw_result_text_area", disabled=True, label_visibility="collapsed"
    )
    raw_download_placeholder = st.empty()

    st.markdown("---") # åˆ†éš”ç·š

    st.subheader("AI æ½¤é£¾å¾Œçµæœ (ç¹é«”ä¸­æ–‡)")
    polished_result_placeholder = st.empty()
    polished_result_placeholder.text_area(
        label="polished_translation_label", value="AI æ½¤é£¾å¾Œçš„ç¿»è­¯çµæœå°‡é¡¯ç¤ºæ–¼æ­¤...", height=250,
        key="polished_result_text_area", disabled=True, label_visibility="collapsed"
    )
    polished_download_placeholder = st.empty()

progress_placeholder = st.empty() # å°‡é€²åº¦æ¢ç§»åˆ°æŒ‰éˆ•ä¸‹æ–¹å…¨åŸŸé¡¯ç¤º

# --- ä¸»åŸ·è¡Œé‚è¼¯ (åŠ å…¥æ½¤é£¾æ­¥é©Ÿ) ---
if translate_button:
    if uploaded_file is None:
        st.warning("è«‹å…ˆä¸Šå‚³ä¸€å€‹æ–‡ä»¶ã€‚")
    else:
        # æ¸…ç©ºå…ˆå‰çµæœ
        raw_download_placeholder.empty()
        polished_download_placeholder.empty()
        progress_placeholder.empty()
        raw_result_placeholder.text_area(label="raw_translation_label", value="è™•ç†ä¸­...", height=250, key="raw_processing", disabled=True, label_visibility="collapsed")
        polished_result_placeholder.text_area(label="polished_translation_label", value="ç­‰å¾…åˆæ­¥ç¿»è­¯å®Œæˆ...", height=250, key="polished_waiting", disabled=True, label_visibility="collapsed")

        with st.spinner(f"æ­£åœ¨è®€å–æª”æ¡ˆ '{uploaded_file.name}'..."):
            extracted_text = extract_text_from_file(uploaded_file)

        if extracted_text is not None and extracted_text.strip():
            st.success("æˆåŠŸå¾æ–‡ä»¶ä¸­æå–è‹±æ–‡æ–‡å­—ï¼")

            # æ­¥é©Ÿ 2: åˆ†å¡Š
            with st.spinner("æ­£åœ¨å°‡æ–‡æœ¬åˆ†å‰²æˆè™•ç†å¡Š..."):
                text_chunks = split_text_into_chunks(extracted_text, MAX_CHARS_PER_CHUNK)
                total_chunks = len(text_chunks)
            if total_chunks == 0: st.warning("æœªèƒ½å°‡æ–‡æœ¬æœ‰æ•ˆåˆ†å‰²æˆå¡Šã€‚"); st.stop()
            st.info(f"æ–‡æœ¬å·²åˆ†å‰²æˆ {total_chunks} å€‹å¡Šé€²è¡Œåˆæ­¥ç¿»è­¯ã€‚")

            # æ­¥é©Ÿ 3: é€å¡Šç¿»è­¯
            translated_chunks = []
            errors_in_translation = False
            progress_bar = progress_placeholder.progress(0)
            status_text = progress_placeholder.text(f"æ­£åœ¨ç¿»è­¯å¡Š 1 / {total_chunks}...")
            fixed_target_language = "ç¹é«”ä¸­æ–‡"

            for i, chunk in enumerate(text_chunks):
                chunk_num = i + 1
                status_text.text(f"åˆæ­¥ç¿»è­¯ï¼šå¡Š {chunk_num} / {total_chunks}...")
                translated_chunk = translate_text(chunk, fixed_target_language)
                if translated_chunk and "[[ç¿»è­¯éŒ¯èª¤:" not in translated_chunk:
                    translated_chunks.append(translated_chunk)
                else:
                    errors_in_translation = True
                    translated_chunks.append(f"\n--- å¡Š {chunk_num} åˆæ­¥ç¿»è­¯å¤±æ•— ---\n{translated_chunk or 'æœªçŸ¥éŒ¯èª¤'}\n---")
                    st.error(f"åˆæ­¥ç¿»è­¯å¡Š {chunk_num} æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚")
                progress_bar.progress(chunk_num / total_chunks)
                if chunk_num < total_chunks: time.sleep(API_CALL_DELAY)
            status_text.text("åˆæ­¥ç¿»è­¯å®Œæˆï¼Œæ­£åœ¨åˆä½µçµæœ...")

            # æ­¥é©Ÿ 4: åˆä½µåˆæ­¥ç¿»è­¯çµæœ
            raw_final_translated_text = "\n\n".join(translated_chunks)
            raw_result_placeholder.text_area(
                label="raw_translation_label_updated", value=raw_final_translated_text, height=250,
                key="raw_result_updated", disabled=False, label_visibility="collapsed"
            )
            if errors_in_translation: st.warning("éƒ¨åˆ†æ–‡æœ¬å¡Šåˆæ­¥ç¿»è­¯å¤±æ•—ï¼Œè«‹æª¢æŸ¥çµæœã€‚")
            else: st.success("æ‰€æœ‰æ–‡æœ¬å¡Šåˆæ­¥ç¿»è­¯å®Œæˆï¼")

            # æä¾›åˆæ­¥ç¿»è­¯çš„ Docx ä¸‹è¼‰
            docx_data_raw, docx_error_raw = create_docx_from_text(raw_final_translated_text, base_filename, "_åˆæ­¥ç¿»è­¯")
            if not docx_error_raw and docx_data_raw['data']:
                raw_download_placeholder.download_button(
                    label=f"ğŸ“¥ ä¸‹è¼‰åˆæ­¥ç¿»è­¯ ({docx_data_raw['name']})", data=docx_data_raw['data'],
                    file_name=docx_data_raw['name'], mime='application/vnd.openxmlformats-officedocument.wordprocessingml.document',
                    key='download_docx_raw'
                )
            else: raw_download_placeholder.error("ç”¢ç”Ÿåˆæ­¥ç¿»è­¯ Docx æ™‚å‡ºéŒ¯ã€‚")

            # --- æ–°å¢æ­¥é©Ÿ 5: æ½¤é£¾ç¿»è­¯çµæœ ---
            status_text.text("æ­£åœ¨é€²è¡Œ AI æ½¤é£¾èˆ‡æ ¡å°...")
            progress_bar.progress(0) # å¯ä»¥é‡è¨­é€²åº¦æ¢æˆ–ç”¨æ–°çš„
            with st.spinner("AI æ­£åœ¨åŠªåŠ›æ½¤é£¾ä¸­ï¼Œè«‹ç¨å€™..."): # å…¨å±€ spinner
                polished_text = polish_translation(raw_final_translated_text, fixed_target_language)
                progress_bar.progress(1) # æ½¤é£¾å®Œæˆ
                status_text.text("AI æ½¤é£¾å®Œæˆï¼")

            if polished_text and "[[æ½¤é£¾éŒ¯èª¤:" not in polished_text:
                polished_result_placeholder.text_area(
                    label="polished_translation_label_updated", value=polished_text, height=250,
                    key="polished_result_updated", disabled=False, label_visibility="collapsed"
                )
                st.success("AI æ½¤é£¾èˆ‡æ ¡å°å®Œæˆï¼")
                # æä¾›æ½¤é£¾å¾Œç¿»è­¯çš„ Docx ä¸‹è¼‰
                docx_data_polished, docx_error_polished = create_docx_from_text(polished_text, base_filename, "_æ½¤é£¾ç‰ˆ")
                if not docx_error_polished and docx_data_polished['data']:
                    polished_download_placeholder.download_button(
                        label=f"ğŸ“¥ ä¸‹è¼‰æ½¤é£¾å¾Œç¿»è­¯ ({docx_data_polished['name']})", data=docx_data_polished['data'],
                        file_name=docx_data_polished['name'], mime='application/vnd.openxmlformats-officedocument.wordprocessingml.document',
                        key='download_docx_polished'
                    )
                else: polished_download_placeholder.error("ç”¢ç”Ÿæ½¤é£¾ç‰ˆ Docx æ™‚å‡ºéŒ¯ã€‚")
            else:
                polished_result_placeholder.text_area(
                    label="polished_translation_label_error", value=f"AI æ½¤é£¾å¤±æ•—æˆ–ç„¡çµæœã€‚\n{polished_text or 'è©³ç´°éŒ¯èª¤è«‹çœ‹ä¸Šæ–¹è¨Šæ¯ã€‚'}", height=250,
                    key="polished_result_error", disabled=False, label_visibility="collapsed"
                )
                st.error("AI æ½¤é£¾æ­¥é©Ÿå¤±æ•—ã€‚")
        # ... (è™•ç†æå–æ–‡å­—å¤±æ•—æˆ–ç‚ºç©ºçš„æƒ…æ³ï¼Œä¿æŒä¸è®Š) ...
        elif extracted_text is not None and not extracted_text.strip():
             st.warning("å¾æª”æ¡ˆä¸­æœªæå–åˆ°ä»»ä½•æœ‰æ•ˆæ–‡å­—å…§å®¹ã€‚")
             raw_result_placeholder.text_area("raw_translation_label_no_text", value="æœªæå–åˆ°æ–‡å­—ã€‚", height=250, key="raw_no_text", disabled=False, label_visibility="collapsed")
        else:
             st.error("ç„¡æ³•å¾æ–‡ä»¶ä¸­æå–æ–‡å­—ã€‚")
             raw_result_placeholder.text_area("raw_translation_label_extract_fail", value="æ–‡ä»¶è®€å–æˆ–æ–‡å­—æå–å¤±æ•—ã€‚", height=250, key="raw_extract_fail", disabled=False, label_visibility="collapsed")